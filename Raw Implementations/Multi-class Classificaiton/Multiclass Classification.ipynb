{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3, Part 1: Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will implement a one-vs-all logistic regression classifier to recognize hand-written digits. There are more than 2 discrete values (namely, 10), so therefore this is a multiclass classification problem. One-vs-all is the technique taught to solve a problem set up like this.\n",
    "\n",
    "For part 1, use:\n",
    "* x3.m (Octave/MATLAB script that steps you through part 1)\n",
    "* ex3data1.mat (Training set of hand-written digits)\n",
    "* lrCostFunction.m (Logistic regression cost function)\n",
    "* oneVsAll.m (Train a one-vs-all multi-class classifier)\n",
    "* predictOneVsAll.m (Predict using a one-vs-all multi-class classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in a .mat format, which is Matlab's native matrix format. We can use the scipy.io module's loadmat function to load .mat files as numpy nd-arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some properties of ex3data1.mat:\n",
      "<class 'dict'>\n",
      "dict_keys(['X', '__version__', '__header__', 'y', '__globals__'])\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_data = loadmat(\"ex3data1.mat\")\n",
    "\n",
    "print(\"Some properties of ex3data1.mat:\")\n",
    "print(type(raw_data))\n",
    "print(raw_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the dictionary has an X and a y key, which, by the looks of the dictionarys, are numpy ndarrays. We can set these to our own variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a look at X matrix: \n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "Shape of a sample (example) of X: \n",
      "(400,) --> 20 x 20 image of digit, flattened to 400x1\n",
      "\n",
      "Total size of training set: 2000000\n"
     ]
    }
   ],
   "source": [
    "# Looking at X\n",
    "print(\"Taking a look at X matrix: \\n{}\\n\".format(raw_data[\"X\"]))\n",
    "\n",
    "# What does a sample of X look like?\n",
    "print(\"Shape of a sample (example) of X: \\n{} --> 20 x 20 image of digit, flattened to 400x1\\n\"\\\n",
    "      .format(raw_data[\"X\"][50, :].shape))\n",
    "\n",
    "# What is the total size of the training set? 5000 samples x 400 features (pixels) == 20000\n",
    "print(\"Total size of training set: {}\".format(raw_data[\"X\"].size))\n",
    "\n",
    "X = raw_data[\"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at y: [[10]\n",
      " [10]\n",
      " [10]\n",
      " ..., \n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]]\n",
      "\n",
      "Shape of y: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Looking at Y\n",
    "print(\"Look at y: {}\\n\".format(raw_data[\"y\"]))\n",
    "\n",
    "# Shape of y\n",
    "print(\"Shape of y: {}\".format(raw_data[\"y\"].shape))\n",
    "\n",
    "y = raw_data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing two samples, reshaping 400x1 to 20x20, viz using plt.imshow()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFIAAABUCAYAAAARdWCGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAA81JREFUeJzt279rFWkUh/FPNGoimEKRQIJdQAQVFQQLK01vkTZoYWGR\nQqvUgmJjIfhHCHYKIvgL1EKbBAtFklRqkxSKEIgkGnWL5TDcK+6yydk46HmaYSYzw+ThO+c97zvc\nnu/fvyvWz6Zf/QC/CyUyiRKZRIlMokQmUSKTKJFJlMgkSmQSJTKJEplEiUyiRCZRIpMokUn0/uoH\ngIGBgVYvii4uLvb82zmVyCRKZBIlMokSmUSJTKJEJtGK9udn9PR0dh2rq6sd282bN4MtW7aAX/lp\nuRKZRKsTGXz58gUMDg6CkZERMD8/D96+fYsfExz73759A8vLy2iSG4netm1bx/lrSXYlMolWJbI7\nESsrK+DAgQPg0qVL4Pjx4+D+/ftgfHwc9Pb2dtzn69evYNeuXWBsbAwMDAyAN2/egNu3b3ecvxYq\nkUm0KpGRxM+fP4PDhw+Da9eugWPHjoFXr16Be/fu4cfaGDVxx44d4PLly2gSGedH7Y0u4ObNm6C/\nv/8/P3slMolWJTJqVNTE69evg/3794Nbt26Bq1evgunpabB169aO+0Qih4aG0NTUOB7Jj5raff1a\nqEQm0apERlJOnToFjhw5Ah4+fAjOnz8P3r9/jyZJ3aN91LzFxcWO7e7du9HMhJ48edKxXU8yK5FJ\ntCqRwcmTJ9Ek7c6dO+Ddu3dg+/btHX+P2hqj/d69e9H0l8PDw2DTpr9zMzU1BS5cuAAWFhbQ1My1\nzGxaKTIa5KNHj4LTp0+DDx8+gNevX4OZmRmwZ88eMDo6CiYmJsC+ffvQCA6R0T7F1DIErod6tZPo\nacOvGuIrYjxLNMSTk5PgzJkzaNqXSNjHjx/RvOo7d+5EM7W8ceMGmgReuXIFzWLF2bNnwaNHj/Dz\nZNZXxA2kVTUyatinT5/AxYsXwd27d9E06ocOHUKT3Ejmy5cv0SxGPH36tOO+Bw8eBOfOnQMnTpxA\ns/ixnlpZiUyiVYnsXnCN/efPn4Nnz57hx0Y8amcsQsTxaLxjQffBgwdoam4sEMf166ESmUSrEhl0\ndxKRwDjeve0+r3vKGMmcnZ0Fc3Nz4MWLF2jegPVQiUyiVX3k/0X8j319fWhmQI8fPwZLS0v/eH31\nkRvIH5HIoHuRI/ajz/yZi0rkBtLKUTub7lE8EhhkvJWVyCT+iERuxDhQiUyiFaP270AlMokSmUSJ\nTKJEJlEikyiRSZTIJEpkEiUyiRKZRIlMokQmUSKTKJFJlMgkSmQSJTKJEplEiUyiRCZRIpMokUmU\nyCRKZBIlMokSmUSJTKJEJlEik/gL9fhKalYwdO4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a86358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFIAAABUCAYAAAARdWCGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAABAVJREFUeJzt2t9rzX8cwPHHNoQWQmJScjFJWgr/gPJjN26WGxn3LnYp\npcSN5Ib5A+RSkixFKXcuFGWFG5KSlJob82Mb4+rdu53vV2x7tX2m1/PmtLOz9dmz517v9+d9TtvP\nnz8ls6d9vi/gXyFFBpEig0iRQaTIIFJkECkyiBQZRIoMIkUGkSKDSJFBpMggUmQQKTKIRfN9AdDZ\n2dnoQ9HR0dG2P70miwwiRQaRIoNIkUE0YrGJpryhVx4nJyfB9+/fQUdHB2hvb5/y2Nb2xzXlt2SR\nQSzIIltLK/z48QO1sI0bN4J169aBgwcPgufPn4Nnz56Bd+/egbGxMdRip0MWGcSCLLLMslWrVk35\nevXq1eDAgQPg6NGjqGUuXrwYteSRkRFw+vRpcOfOHdTipzMzs8ggFlSRZdVdu3YtOH/+PFizZg1Y\nv3496O7u/t+fL6WVGVpef/bsWdQCb926BRYt+ns9WWQQbU34ENWfDi3KTFu6dCk4deoUOHHiBGo5\n3759Qy1veHgY3L17F3VVLqv4kSNHUAt/8uQJ2Lt3L+pMzUOLOaTRM7L1v+X48eOgr68PfPnyBXz4\n8AFs2bIFvH79Gpw8eRI8evQItdxS2tatW1FX+c7OzhlfaxYZRKOLLKv0vn37wMDAAOqMGxoaAlev\nXp3y/XLn8vTpU7Bs2bIpv7eUXmZqWa0nJiZmfK1ZZBCNLLKs0itXrkSdjWXf9/LlS3DmzBnw6tUr\n8PjxY9TCWh9LcV1dXah3NmXWln3pTGikyPIHb9u2DfT09IDx8XFw8+ZN8PbtW7BkyRJUIa23duX5\nMiLOnTuHKvTixYvg/v37yEOLeaWRRZZFZvv27WDz5s2oBT58+BB1g1026mUktG68d+3aBQ4fPgz2\n7NkDLl++DAYHB6f8fBY5jzSyyEKZieXAtmyoy3amPF9mYDlG27lzJ2qBx44dQy3typUrqLNyNiUW\nssggGllkmXn37t0D165dA/39/WD//v2oJR46dAh1Fd6xYwfYsGED+PjxI7h+/Tq4cOEC+Pz5M6Z3\nXPY7ssggGn2MVvaTmzZtAjdu3EBdzb9+/QqWL1+OWtb79+/BpUuXwIMHD8CLFy9QbxH/dibmMdoc\n0ugiC2VfuXv3btDb24t65zM6OgrevHmD+jbr7du38d8Cp/tBgCxyDlkQRRZKmYUVK1ag7idLmaW4\nmRbYShY5hzRyH/k7WlfZT58+oRbXuh+cbYnTIYsMYkEV2VrYbO6No8kig2jEqv0vkEUGkSKDSJFB\npMggUmQQKTKIFBlEigwiRQaRIoNIkUGkyCBSZBApMogUGUSKDCJFBpEig0iRQaTIIFJkECkyiBQZ\nRIoMIkUGkSKDSJFBpMggUmQQvwCHRU36QqGrbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10452de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Visualizing two samples, reshaping 400x1 to 20x20, viz using plt.imshow()\")\n",
    "\n",
    "single_sample = np.expand_dims(X[1324], axis=0)\n",
    "single_sample = single_sample.reshape((20, 20))\n",
    "\n",
    "another_sample = np.expand_dims(X[321], axis=0)\n",
    "another_sample = another_sample.reshape((20, 20))\n",
    "\n",
    "plt.figure(figsize = (0.5,0.5))\n",
    "plt.gray()\n",
    "plt.imshow(single_sample, interpolation='nearest')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (0.5,0.5))\n",
    "plt.imshow(another_sample, interpolation='nearest')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid, Cost Function, and Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid: Classic implementation of a vectorized sigmoid function.\n",
    "\n",
    "Cost Function: This is the standard cost function -- it takes in whatever parameters you wish to process, the X array from which a np.dot() operation will give you your predictions, and the y array to compare against. The cost function in logistic is different from linear regression in the sense that it uses log to judge our predictions after they are transformed by the sigmoid function. Since the sigmoid function returns 0.0 <= x <= 1.0, we know that an application of log will result in 0 if our prediction is 1, and will result in 0 if we subtract 1 from what the logarithm outputs when our actual prediction is 0. It is also regularized due to the 400 parameters per example.\n",
    "\n",
    "Gradient: To use advanced optimization methods, we have to provide a streamlined way for the API of that module to calculate the derivative(s) of the cost function w.r.t each parameter. We provide the gradients for any set of parameters you want to observe, and also it is regularized because of the high feature count (same way GD is regularized when it changes each parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining a regularized cost function to use for an advanced optimization method -- note y is an array\n",
    "def cost_function(parameters, X, y, reg_parameter):\n",
    "    # Part 1: Calculating cost\n",
    "    y_vector = y.ravel()\n",
    "    positive_class = -1 * np.multiply(y, np.log(sigmoid(np.dot(X, parameters.T))))\n",
    "    negative_class = np.multiply((1 - y), np.log(1 - sigmoid(np.dot(X, parameters.T))))\n",
    "    \n",
    "    # Regularization inner term excludes Theta0\n",
    "    inner_reg_term = np.power(parameters[1:], 2)\n",
    "    reg_term = np.sum(inner_reg_term) * (reg_parameter / (2 * len(X)))\n",
    "    total_cost = (np.sum(positive_class - negative_class) / len(X)) + reg_term\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(parameters, X, y, reg_parameter):\n",
    "    pred_act_diff = sigmoid(np.dot(X, parameters.T)) - y\n",
    "    \n",
    "    gradient = ((1 / len(X)) * np.sum(X * pred_act_diff.T, axis=0))\n",
    "    gradient[1:] = gradient[1:] + ((reg_parameter / len(X)) * parameters[1:])\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cost: 2.534819\n",
      "Actual cost: 2.534819396109744\n",
      "\n",
      "Expected values: 0.146561, -0.548558, 0.724722, 1.398003\n",
      "Actual values: [ 0.14656137 -0.54855841  0.72472227  1.39800296]\n"
     ]
    }
   ],
   "source": [
    "# Testing our cost_function\n",
    "theta_t = np.array([-2.0, -1.0, 1, 2.0])\n",
    "example_array = np.array([[0.1, 0.6, 1.1], [0.2, 0.7, 1.2], [0.3, 0.8, 1.3], [0.4, 0.9, 1.4], [0.5, 1.0, 1.5]])\n",
    "X_t = np.append(np.ones((5, 1)), example_array, axis=1)\n",
    "y_t = np.array([[1., 0., 1., 0., 1.]])\n",
    "lambda_t = 3\n",
    "print(\"Expected cost: 2.534819\\nActual cost: {}\\n\".format(cost_function(theta_t, X_t, y_t, lambda_t)))\n",
    "print(\"Expected values: 0.146561, -0.548558, 0.724722, 1.398003\\nActual values: {}\".format(\\\n",
    "                                                        gradient(theta_t, X_t, y_t, lambda_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimization Method\n",
    "\n",
    "Here, we utilize scipy.optimize module's minimize function. This function takes in the fun=cost_function, x0=parameters, args=(X_mod, y_mod, lambda_var), method=\"TNC\", jac=gradient. With the gradient and cost_function armed, the advanced optimization method is able to iteratively come up with a new set of transformed parameters with the graduent and then after judge them with the cost_function until it has reached a point where it is completed. \n",
    "\n",
    "### Note: Idea of using scipy.optimize was discovered in https://github.com/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ML-Exercise3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# NOTE: \"0\" is labeled 10\n",
    "def one_vs_all(X, y, num_labels, lambda_var):\n",
    "    # scipy.optimize is not aware that our dataset does NOT contain 1's --> add the intercept column\n",
    "    X_mod = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    \n",
    "    parameter_count = X_mod.shape[1]\n",
    "    classifier_parameters = np.zeros((num_labels, parameter_count))\n",
    "    \n",
    "    for i in range(1, num_labels + 1):\n",
    "        y_mod = np.expand_dims(np.array([ 1 if label == i else 0 for label in y]), axis=1).T\n",
    "        parameters = np.zeros(X_mod.shape[1])\n",
    "        \n",
    "        result = minimize(fun=cost_function, x0=parameters, args=(X_mod, y_mod, lambda_var), method=\"TNC\", jac=gradient)\n",
    "        \n",
    "        # Otherwise, element 0 would be empty -- inspiration taken form jdwittaneur, all other work is my own.\n",
    "        classifier_parameters[i-1, :] = result.x\n",
    "        \n",
    "    return classifier_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = one_vs_all(X, y, 10, 1)\n",
    "result_shape = result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_all(X, y, result):\n",
    "    X_mod = np.append(np.ones((len(X), 1)), X, axis=1)\n",
    "\n",
    "    predictions = sigmoid(np.dot(X_mod, result.T))\n",
    "    prediction_list = np.array([np.argmax(transformed_datapoint) + 1 for transformed_datapoint in predictions])\n",
    "    prediction_list = np.expand_dims(prediction_list, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(prediction_list == y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 94.46%\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy: {:.2f}%\".format(predict_all(X, y, result) * 100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
