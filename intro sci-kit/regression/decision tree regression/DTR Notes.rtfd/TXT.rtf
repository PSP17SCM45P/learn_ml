{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww26020\viewh13800\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Decision Tree Regression\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
CART: Classification and Regression Trees\
\'97 Think: Random forests\
\
We will talk about regression trees, though classification trees can be looked at later.\
\
Consider a scatter plot we will use to make the decision tree. We will be looking at two independent variables.\
In this scatter plot, the Decision Tree Regression algorithm will be making splits in the dataset with respect to the independent variables,\
splitting them in such a way that gives the algorithm more information about our points. A split will boost the understanding of the dataset\
for the algorithm, and splits will be continuously made until no more information can be gained \'97 this concept is more formally called\
information entropy. \
\
Information entropy  tells how much information there is in an event. The more random a certain event is, the more\
information it will contain. A \'93split\'94 is an act of information gain, a measure of the probability with which a certain result is expected to happen.\
The algorithm stops when a split gives perhaps less than 5% information gain in a sector. Each split is a LEAF.\
\
\
Let\'92s rewind, and create these splits one by one.\
\
\
For each split, there\'92s an entry in the decision tree that corresponds to what information we just gained by making that split. Each branch represents EITHER \'93Yes\'94 or \'93No\'94 for the next split to go down to. The next split might be on X2 < 30,\
on a split before the X1 < 40 mark, putting the X2 < 30 tree entry on the \'93yes\'94 side (branch) of the X1 < 40 tree entry. We keep doing splits and therefore keep adding information to our tree until there is no more possible (or too little) information\
gain via information entropy.\
\
At the end, each LEAF has its own points. We look at every leaf, average its projected Y values, and then assign that leaf the average to spit back out when an input comes in that falls in that LEAF.\
\
Ex: Split 4 = 34.5   X1 10 < # < 30  X2 -13 < 19\
\
X1 = 17, X2 = 8  \'97\'97 > PREDICTION: 34.5 (Y)}